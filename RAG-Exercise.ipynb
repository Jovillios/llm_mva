{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CPGVVOEHZwdb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.48.2)\n",
      "Requirement already satisfied: wikipedia in /opt/conda/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.12/site-packages (from wikipedia) (4.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4->wikipedia) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wikipedia\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(page_titles):\n",
    "    \"\"\"\n",
    "    Extracts Wikipedia pages and stores them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        page_titles: A list of Wikipedia page titles to extract.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the text of each Wikipedia page.\n",
    "    \"\"\"\n",
    "\n",
    "    page_data = {}\n",
    "    for title in page_titles:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            content = page.content.strip()\n",
    "            content = content.replace(\"\\n\", \"\")\n",
    "            page_data[page.title] = content\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            print(f\"Page '{title}' not found.\")\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            print(f\"Disambiguation error for '{title}': {e.options}\")\n",
    "\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_titles = [\n",
    "               \"Roger Apéry\",\n",
    "               \"Owen Willans Richardson\",\n",
    "               \"Otto Sackur\",\n",
    "               \"Ludvig Lorenz\",\n",
    "               \"Klaus von Klitzing\",\n",
    "               \"Henri Victor Regnault\",\n",
    "               \"Erwin Madelung\",\n",
    "              ]\n",
    "\n",
    "# Uncomment the next line to scroll through Wikipedia\n",
    "# wikipedia_data = extract_wikipedia_pages(page_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary using `json.dump()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('wikipedia_data.json', 'w') as f:\n",
    "#     json.dump(wikipedia_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dictionary using `json.load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikipedia_data.json', 'r') as f:\n",
    "    wikipedia_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3107\n",
      "3455\n",
      "1683\n",
      "1873\n",
      "1762\n",
      "3431\n",
      "1487\n"
     ]
    }
   ],
   "source": [
    "for doc in wikipedia_data:\n",
    "    print(len(wikipedia_data[doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load just the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model_max_length = tokenizer.model_max_length\n",
    "model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello [SEP] how are you? [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode([\"hello\", \"how are you?\"])\n",
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01234',\n",
       " '34567',\n",
       " '67891',\n",
       " '91011',\n",
       " '11121',\n",
       " '21314',\n",
       " '14151',\n",
       " '51617',\n",
       " '17181',\n",
       " '819']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_splitting(text, chunk_length = 300, chunk_overlap = 100):\n",
    "    return [text[i:chunk_length+i] for i in range(0, len(text), chunk_length-chunk_overlap)]\n",
    "\n",
    "text_splitting(\"\".join([str(x) for x in range(20)]), 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Roger Apéry (French: [apeʁi]; 14 November 1916, Rouen – 18 December 1994, Caen) was a French mathematician most remembered for Apéry's theorem, which states that ζ(3) is an irrational number. Here, ζ(s) denotes the Riemann zeta function.== Biography ==Apéry was born in Rouen in 1916 to a French moth\",\n",
       " 's) denotes the Riemann zeta function.== Biography ==Apéry was born in Rouen in 1916 to a French mother and Greek father. His childhood was spent in Lille until 1926, when the family moved to Paris, where he studied at the Lycée Ledru-Rollin and the Lycée Louis-le-Grand.  He was admitted  at the Écol',\n",
       " 'ere he studied at the Lycée Ledru-Rollin and the Lycée Louis-le-Grand.  He was admitted  at the École normale supérieure in 1935.  His studies were interrupted at the start of World War II; he was mobilized in September 1939, taken prisoner of war in June 1940, repatriated with pleurisy in June 1941']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_data_splits = {}\n",
    "\n",
    "for doc in wikipedia_data.keys():\n",
    "    wikipedia_data_splits[doc] = text_splitting(wikipedia_data[doc])\n",
    "    #wikipedia_data_splits[doc] = text_splitting_paragraph(wikipedia_data[doc])\n",
    "\n",
    "first_key = page_titles[0]\n",
    "wikipedia_data_splits[first_key][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 18, 12.571428571428571)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_doc = min(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits)\n",
    "max_doc = max(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits)\n",
    "av_doc = sum(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits) / len(wikipedia_data_splits)\n",
    "\n",
    "min_doc,max_doc,av_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the embedder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "output_dim = outputs.last_hidden_state.size(2)\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedder needs to know whether the document is a document or a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunk_list, doc_type=\"document\"):\n",
    "    encoded_docs = tokenizer([\"search_{}: {}\".format(doc_type, chunk) for chunk in chunk_list],\n",
    "                                 padding = True,\n",
    "                                 return_tensors=\"pt\")\n",
    "    output = model(**encoded_docs) # (batch, input_length, output_dim)\n",
    "    token_embeddings = output.last_hidden_state\n",
    "    output_embeddings = torch.sum(token_embeddings, 1)\n",
    "    output_embeddings = F.normalize(output_embeddings, p=2, dim=1)\n",
    "    return output_embeddings # (batch, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed([\"hello\", \"another document\", \"and another one\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: chunks may lack context. The ideal of `contextual embeddings` is to ask an LLM to write some context about the chunk (given the full document and the chunk), and to embed the chunk together with the context.\n",
    "Implement this idea here (choose a simple enough model and the appropriate task!).\n",
    "> Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roger Apéry 16\n",
      "Owen Richardson 18\n",
      "Otto Sackur 9\n",
      "Ludvig Lorenz 10\n",
      "Klaus von Klitzing 9\n",
      "Henri Victor Regnault 18\n",
      "Erwin Madelung 8\n"
     ]
    }
   ],
   "source": [
    "def populate_database(dic_splits, batch_size = 1):\n",
    "    n_chunks = sum([len(dic_splits[doc]) for doc in dic_splits])\n",
    "    vectorial_database = torch.zeros([n_chunks, output_dim], requires_grad = False).to(device)\n",
    "    chunk_list = []\n",
    "    n = 0\n",
    "    for i, doc in enumerate(dic_splits):\n",
    "        split_list = dic_splits[doc]\n",
    "        print(doc, len(split_list))\n",
    "        for x in range(0, len(split_list), batch_size):\n",
    "            batch = split_list[x: x+batch_size]\n",
    "            chunk_list.append(batch)\n",
    "            vectorial_database[n:n+len(batch)] = embed(batch, \"document\")\n",
    "            n += len(batch)\n",
    "    return chunk_list, vectorial_database \n",
    "\n",
    "chunk_list, vectorial_database = populate_database(wikipedia_data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vectorial database using `torch.save()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vectorial_database, 'vectorial_database.pth')\n",
    "\n",
    "with open('chunk_list.json', 'w') as f:\n",
    "    json.dump(chunk_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database using `torch.load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorial_database = torch.load('vectorial_database.pth')\n",
    "vectorial_database.to(device)\n",
    "vectorial_database.requires_grad_(False)\n",
    "\n",
    "with open('chunk_list.json', 'r') as f:\n",
    "    chunk_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, torch.Size([88, 384]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_list), vectorial_database.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0406,  0.0437, -0.0288, -0.0089, -0.0150], device='cuda:0') [\"Roger Apéry (French: [apeʁi]; 14 November 1916, Rouen – 18 December 1994, Caen) was a French mathematician most remembered for Apéry's theorem, which states that ζ(3) is an irrational number. Here, ζ(s) denotes the Riemann zeta function.== Biography ==Apéry was born in Rouen in 1916 to a French moth\"]\n",
      "tensor([-0.0140,  0.0815, -0.0226, -0.0117,  0.0452], device='cuda:0') ['s) denotes the Riemann zeta function.== Biography ==Apéry was born in Rouen in 1916 to a French mother and Greek father. His childhood was spent in Lille until 1926, when the family moved to Paris, where he studied at the Lycée Ledru-Rollin and the Lycée Louis-le-Grand.  He was admitted  at the Écol']\n",
      "tensor([ 0.0196,  0.0287, -0.0544,  0.0098, -0.0017], device='cuda:0') ['ere he studied at the Lycée Ledru-Rollin and the Lycée Louis-le-Grand.  He was admitted  at the École normale supérieure in 1935.  His studies were interrupted at the start of World War II; he was mobilized in September 1939, taken prisoner of war in June 1940, repatriated with pleurisy in June 1941']\n",
      "tensor([-0.0451,  0.0146, -0.0351,  0.0092, -0.0861], device='cuda:0') ['ilized in September 1939, taken prisoner of war in June 1940, repatriated with pleurisy in June 1941, and hospitalized until August 1941. He wrote his doctoral thesis in algebraic geometry under the direction of Paul Dubreil and René Garnier in 1947.In 1947 Apéry was appointed Maître de conférences ']\n",
      "tensor([-0.0941, -0.0131,  0.0109,  0.0063, -0.0961], device='cuda:0') ['irection of Paul Dubreil and René Garnier in 1947.In 1947 Apéry was appointed Maître de conférences (lecturer) at the University of Rennes. In 1949 he was appointed Professor at the University of Caen, where he remained until his retirement.In 1979 he published an unexpected proof of the irrationali']\n",
      "tensor([-0.0573,  0.0642, -0.0582,  0.1400, -0.0297], device='cuda:0') [', where he remained until his retirement.In 1979 he published an unexpected proof of the irrationality of ζ(3), which is the sum of the inverses of the cubes of the positive integers. An indication of the difficulty is that the corresponding problem for other odd powers remains unsolved. Nevertheles']\n",
      "tensor([-0.1706,  0.0305,  0.0359,  0.0810,  0.0035], device='cuda:0') [' the difficulty is that the corresponding problem for other odd powers remains unsolved. Nevertheless, many mathematicians have since worked on the so-called Apéry sequences to seek alternative proofs that might apply to other odd powers (Frits Beukers, Alfred van der Poorten, Marc Prévost, Keith Ba']\n",
      "tensor([-0.0215,  0.0363, -0.0475,  0.0384,  0.0550], device='cuda:0') [' that might apply to other odd powers (Frits Beukers, Alfred van der Poorten, Marc Prévost, Keith Ball, Tanguy Rivoal, Wadim Zudilin, and others).Apéry was active in politics and for a few years in the 1960s was president of the Calvados Radical Party of the Left. He abandoned politics after the ref']\n",
      "tensor([ 0.0134,  0.0384, -0.0150,  0.0524,  0.0222], device='cuda:0') ['e 1960s was president of the Calvados Radical Party of the Left. He abandoned politics after the reforms instituted by Edgar Faure after the 1968 revolt, when he realised that university life was running against the tradition he had always upheld.== Personal life ==Apéry married in 1947 and had thre']\n",
      "tensor([-0.0357,  0.0354,  0.0140, -0.0017, -0.0047], device='cuda:0') [\"ing against the tradition he had always upheld.== Personal life ==Apéry married in 1947 and had three sons, including mathematician François Apéry. His first marriage ended in divorce in 1971. He then remarried in 1972 and divorced in 1977.In 1994, Apéry died from Parkinson's disease after a long il\"]\n",
      "tensor([-0.0324,  0.0570,  0.0161, -0.0176, -0.0074], device='cuda:0') [\" remarried in 1972 and divorced in 1977.In 1994, Apéry died from Parkinson's disease after a long illness in Caen. He was buried next to his parents at the Père Lachaise Cemetery in Paris. His tombstone has a mathematical inscription stating his theorem.                    1        +                \"]\n",
      "tensor([-0.0927,  0.0700,  0.0138,  0.0439, -0.1097], device='cuda:0') ['ne has a mathematical inscription stating his theorem.                    1        +                              1            8                          +                              1            27                          +                              1            64                          + ']\n",
      "tensor([-0.0740,  0.0926,  0.0288,  0.1137, -0.0342], device='cuda:0') ['                          +                              1            64                          +        ⋯        ≠                              p            q                                {\\\\displaystyle 1+{\\\\frac {1}{8}}+{\\\\frac {1}{27}}+{\\\\frac {1}{64}}+\\\\cdots \\\\neq {\\\\frac {p}{q}}}  == See also ==']\n",
      "tensor([-0.1319,  0.0438, -0.0338,  0.1194, -0.0331], device='cuda:0') ['aystyle 1+{\\\\frac {1}{8}}+{\\\\frac {1}{27}}+{\\\\frac {1}{64}}+\\\\cdots \\\\neq {\\\\frac {p}{q}}}  == See also ==Apéry\\'s constantBasel problem== External links ==Apéry, François (1996). \"Roger Apéry, 1916-1994: A Radical Mathematician\". The Mathematical Intelligencer. 18 (2): 54–61. doi:10.1007/BF03027295. S2CID']\n",
      "tensor([-0.0607,  0.0336,  0.0074,  0.0763,  0.0489], device='cuda:0') ['Radical Mathematician\". The Mathematical Intelligencer. 18 (2): 54–61. doi:10.1007/BF03027295. S2CID 120113351.van der Poorten, Alfred (1979). \"A proof that Euler missed ... Apéry\\'s proof of the irrationality of ζ(3)\". The Mathematical Intelligencer. 1 (4): 195–203. doi:10.1007/BF03028234. S2CID 121']\n",
      "tensor([-0.0448, -0.0178,  0.0003,  0.0595,  0.0006], device='cuda:0') ['ionality of ζ(3)\". The Mathematical Intelligencer. 1 (4): 195–203. doi:10.1007/BF03028234. S2CID 121589323.']\n",
      "tensor([-0.1200,  0.0288, -0.0543,  0.0940,  0.0235], device='cuda:0') [\"Sir Owen Willans Richardson (26 April 1879 – 15 February 1959) was a British physicist who won the Nobel Prize in Physics in 1928 for his work on thermionic emission, which led to Richardson's law.== Biography ==Richardson was born in Dewsbury, Yorkshire, England, the son of Joshua Henry and Charlot\"]\n",
      "tensor([-0.0728, -0.0260, -0.0533,  0.0900, -0.0116], device='cuda:0') ['Biography ==Richardson was born in Dewsbury, Yorkshire, England, the son of Joshua Henry and Charlotte Maria Richardson. He was educated at Batley Grammar School and Trinity College, Cambridge, where he gained First Class Honours in Natural Sciences. He then got a DSc from University College London ']\n",
      "tensor([-0.0794,  0.0135,  0.0028,  0.0811, -0.0019], device='cuda:0') ['he gained First Class Honours in Natural Sciences. He then got a DSc from University College London in 1904.After graduating in 1900, he began researching the emission of electricity from hot bodies at the Cavendish Laboratory in Cambridge, and in October 1902 he was made a fellow at Trinity. In 190']\n",
      "tensor([-0.1603,  0.0160, -0.0340,  0.0700, -0.0166], device='cuda:0') ['t the Cavendish Laboratory in Cambridge, and in October 1902 he was made a fellow at Trinity. In 1901, he demonstrated that the current from a heated wire seemed to depend exponentially on the temperature of the wire with a mathematical form similar to the Arrhenius equation. This became known as Ri']\n"
     ]
    }
   ],
   "source": [
    "for i, embedding_vector in enumerate(vectorial_database[:20]):\n",
    "    print(embedding_vector[:5], chunk_list[i][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(query_embeddings, doc_embeddings):\n",
    "    return query_embeddings @ doc_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6542],\n",
      "        [0.4483]])\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = embed([\n",
    "    \"What is TSNE?\",\n",
    "    \"Who is Laurens van der Maaten?\",\n",
    "], \"query\")\n",
    "\n",
    "doc_embeddings = embed([\n",
    "    \"TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\n",
    "], \"document\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(similarity(query_embeddings, doc_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, \n",
    "             vectorial_database = vectorial_database, \n",
    "             chunk_list = chunk_list, \n",
    "             topk = 5,\n",
    "             verbose = False):\n",
    "    query_embedding = embed([query], \"query\")\n",
    "    similarity_scores = similarity(query_embeddings.to(device), vectorial_database.to(device))\n",
    "    return torch.topk(similarity_scores, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 243.89 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "559 ms ± 576 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "retrieve(\"What did Erwin Madelung study?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: The similarity measure is based on embeddings. A completely different approach is `lexical matching`, meaning by matching keywords from the query to the documents. It is based on `TF-IDF (Term Frequency-Inverse Document Frequency)`, as follows:\n",
    "* Compute TF-IDF for each chunk\n",
    "* BM25 returns the 25 most relevant chunks based on their TF-IDF match to the query\n",
    "\n",
    "Implement this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**:\n",
    "A `reranker` is (yet another) LLM which looks at the query and some chunks and ranks them by relevance. \n",
    "Implement this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information: Claude combines BM25 with similarity measures as follows:\n",
    "* Use BM25 to retrieve 25 chunks\n",
    "* independently, use similarity measure on embeddings to retrieve 25 chunks\n",
    "* Use a reranker to combine and deduplicate the obtained 50 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative retrieval: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return all the closest globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "def retrieve_SVM(query, \n",
    "             vectorial_database = vectorial_database, \n",
    "             chunk_list = chunk_list, \n",
    "             topk = 5):\n",
    "    query_embedding = embed([query], \"query\").to(device)\n",
    "    x = np.concatenate([query_embedding.detach().cpu().numpy(), vectorial_database.detach().cpu().numpy()])\n",
    "    y = np.zeros(vectorial_database.size(0) + 1)\n",
    "    y[0] = 1 # we have a single positive example\n",
    "\n",
    "    clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=10000, tol=1e-6, C=0.1, dual=\"auto\")\n",
    "    clf.fit(x, y)\n",
    "    similarities = clf.decision_function(x)\n",
    "    sorted_ix = np.argsort(-similarities)\n",
    "    for k in sorted_ix[1:topk+1]:\n",
    "        print(f\"Score: {similarities[k]:.4f}\\nText:\\n\", chunk_list[k-1], \"\\n\")\n",
    "    return \"\\n\".join([chunk_list[k-1] for k in sorted_ix[1:topk+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0703\n",
      "Text:\n",
      " ['erlag, Berlin 1922. subsequent editions: 1925, 1936, 1950, 1953, 1957, 1964.== References ==== External links ==Works by or about Erwin Madelung at the Internet ArchiveLiterature by and about Erwin Madelung in the German National Library cataloguePortrait drawing at Frankfurt University'] \n",
      "\n",
      "Score: -0.2228\n",
      "Text:\n",
      " ['Erwin Madelung (18 May 1881 – 1 August 1972) was a German physicist.He was born in 1881 in Bonn. His father was the surgeon Otto Wilhelm Madelung. He earned a doctorate in 1905 from the University of Göttingen, specializing in crystal structure, and eventually became a professor. It was during this '] \n",
      "\n",
      "Score: -0.2334\n",
      "Text:\n",
      " ['erences ==== External links ==Works by or about Otto Sackur at the Internet Archive'] \n",
      "\n",
      "Score: -0.3141\n",
      "Text:\n",
      " ['                          +                              1            64                          +        ⋯        ≠                              p            q                                {\\\\displaystyle 1+{\\\\frac {1}{8}}+{\\\\frac {1}{27}}+{\\\\frac {1}{64}}+\\\\cdots \\\\neq {\\\\frac {p}{q}}}  == See also =='] \n",
      "\n",
      "Score: -0.3486\n",
      "Text:\n",
      " ['islicenus. 1877== References =='] \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretrieve_SVM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat did Erwin Madelung study?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 19\u001b[0m, in \u001b[0;36mretrieve_SVM\u001b[0;34m(query, vectorial_database, chunk_list, topk)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sorted_ix[\u001b[38;5;241m1\u001b[39m:topk\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarities[k]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mText:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, chunk_list[k\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msorted_ix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "retrieve_SVM(\"What did Erwin Madelung study?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does **extractive** question answering, meaning it can only points to the answer in the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a44df65d5e4904bb9b82da75eacb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/835 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8e4f31e2be47988d51628dd8671801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/326M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f312f80ce24a8d8c34c4423b430dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c5343229534bf1a2570a0189e28c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bde403d1c034b2297222105689f6e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c07234f7bc74bd4a9e13f6311327480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5110fa2ca0014ec89031deac7abbd614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "\n",
    "QA = pipeline('question-answering', model=model_name, tokenizer=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(prompt):\n",
    "    topk_chunks = retrieve(prompt)\n",
    "#     topk_chunks = retrieve_SVM(prompt)\n",
    "    return QA(question=prompt, context=topk_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arguments can't be understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat did Erwin Madelung study?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36mquery\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      2\u001b[0m     topk_chunks \u001b[38;5;241m=\u001b[39m retrieve(prompt)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     topk_chunks = retrieve_SVM(prompt)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:396\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[1;32m    391\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:208\u001b[0m, in \u001b[0;36mQuestionAnsweringArgumentHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]}]\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be understood\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments can't be understood"
     ]
    }
   ],
   "source": [
    "query(\"What did Erwin Madelung study?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
